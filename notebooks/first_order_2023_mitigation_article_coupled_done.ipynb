{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ed8868",
   "metadata": {},
   "source": [
    "# Equation\n",
    "# $$ \\dot{v} (t)= g$$\n",
    "\n",
    "## Initial condition:\n",
    "# $$v(0) = 0$$\n",
    "\n",
    "# Solutions\n",
    "# $$v(t) = gt t$$\n",
    "# $$u(t) = \\mbox{Real}z(t) = \\cos t$$\n",
    "# $$v(t) = \\mbox{Im}z(t)   = \\sin t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd15ca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 09:49:59.054994: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 09:49:59.141084: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-12 09:49:59.162207: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-12 09:49:59.574111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/home/tshegofatso/miniconda3/envs/tf_38gpu/lib/\n",
      "2023-01-12 09:49:59.574150: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/home/tshegofatso/miniconda3/envs/tf_38gpu/lib/\n",
      "2023-01-12 09:49:59.574153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow_probability as tfp\n",
    "from tqdm.notebook import tnrange\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d380bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "    print('WARNING: GPU device not found.')\n",
    "else:\n",
    "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eef538",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Inputs:\n",
    "    \n",
    "    units : number of neurons\n",
    "    initialization = 0,1\n",
    "           if activations are:\n",
    "           init =0(He): ReLU, Leaky ReLU, ELU, GELU, Swish, Mish\n",
    "           init =1(Xavier_Glorot): None, tanh, sigmoid, softmax \n",
    "           \n",
    "    activation: activation to be use either string or callable\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "            returns activation(wx+b) with appropriate initialization\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, units = 20, initialization=None, activation = None): #None = gives basic activation\n",
    "        super(MyDense, self).__init__()\n",
    "        self.units = units   \n",
    "        self.init = initialization\n",
    "        \n",
    "        if activation == 'mish':\n",
    "            self.activation = tf.keras.activations.get(self.mish_activation)\n",
    "        else:\n",
    "            self.activation = tf.keras.activations.get(activation)\n",
    "       \n",
    "    @tf.function\n",
    "    def mish_activation(self, x):\n",
    "#         print(f'called_mish')\n",
    "    #     mish(x) = xtanh(softsplus(x))  it needs  He initialization\n",
    "        return x*tf.math.tanh(tf.nn.softplus(x))       \n",
    "    \n",
    "    \n",
    "# We get them from the shape of w= shape\n",
    "# shape[0] : number of inputs to the neuron\n",
    "# shape[1] : number of output from the neuron\n",
    "#     @tf.function(reduce_retracing=True)\n",
    "    def Xavier_Glorot_initializer(self,shape, dtype=tf.float32):\n",
    "        '''\n",
    "        Xavier Glorot: initialization must be used for the following activations:\n",
    "        None, tanh, sigmoid, softmax\n",
    "\n",
    "       input: \n",
    "            shape = [number of columns, units]\n",
    "\n",
    "       output:\n",
    "               samples normal distribution of mean = 0 stddev = sqrt(2./inputs_shape+units)\n",
    "        '''\n",
    "#         print('Initial weight with  Xavier Glorot ')\n",
    "        \n",
    "        shape_to_float = tf.cast(shape, dtype=tf.float32)\n",
    "        fanin = shape_to_float[0]\n",
    "        fanout = shape_to_float[1]\n",
    "        stddev = tf.sqrt(2. /(fanin+fanout)  )\n",
    "        return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "    \n",
    "#     @tf.function(reduce_retracing=True) \n",
    "    def He_initializer(self,shape,dtype = tf.float32):\n",
    "        '''\n",
    "        input: \n",
    "                shape = [number of columns, units]\n",
    "\n",
    "        He (aka He normal)initialization must be used for the following activations:\n",
    "        ReLU, Leaky ReLU, ELU, GELU, Swish, Mish\n",
    "\n",
    "        He initialization:\n",
    "        random normal with variance and mean of 2/fanin = 2/no. of inputs = 2/inputs\n",
    "        '''\n",
    "        shape_to_float = tf.cast(shape, dtype=tf.float32)\n",
    "#         print(f'INIT shape = {shape}, dtype = {shape.dtype}')\n",
    "#         fanin = tf.constant(shape[0], dtype = tf.float32)\n",
    "        fanin = shape_to_float[0]\n",
    "#         print(f'fanin = {fanin}')\n",
    "        stddev = tf.sqrt(2. /fanin)\n",
    "#         print(f'stddev = {stddev}')\n",
    "#         print('Initial weight with  He')\n",
    "        return tf.random.normal(shape, stddev=stddev, dtype=dtype)        \n",
    "\n",
    "#     Initialize kernel aka w\n",
    "    def init_w(self, input_shape):\n",
    "        shape = [input_shape[-1], self.units]\n",
    "        shape = tf.constant(shape, dtype=tf.int32)\n",
    "\n",
    "        if self.init == 0:\n",
    "#             Initialize with He : Relu and variants activation functions\n",
    "            value = self.He_initializer(shape)\n",
    "        else:\n",
    "#             Initialize with Xavier_Glorot : None, tanh, sigmoid, softmax activation functions\n",
    "            value = self.Xavier_Glorot_initializer(shape)\n",
    "            \n",
    "        return tf.Variable(name = 'kernel', initial_value=value ,trainable = True )\n",
    "\n",
    "#     Initialize biase aka b\n",
    "    def init_b(self):\n",
    "#         value = tf.zeros(self.units)  # originsl\n",
    "        value = tf.ones(self.units)\n",
    "        return tf.Variable(name = 'bias', initial_value=value ,trainable = True )    \n",
    "       \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.init_w(input_shape)\n",
    "        self.b = self.init_b()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        out_put = self.activation(tf.matmul(inputs, self.w) + self.b)\n",
    "        return out_put\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"weight\": self.w,\n",
    "            \"biase\": self.b,\n",
    "            })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bace8d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5845/2390518753.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mZ_approx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_of_hid_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits_per_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_approx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_of_hid_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_of_hid_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits_per_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits_per_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class Z_approx(tf.keras.Model):\n",
    "    def __init__(self,v0,num_of_hid_layers = None, units_per_layer = None, num_out=1, initialization=None, activation=None):\n",
    "        super(Z_approx,self).__init__()\n",
    "        self.num_of_hid_layers = num_of_hid_layers\n",
    "        self.units_per_layer = units_per_layer\n",
    "        self.num_out = num_out\n",
    "        self.init = initialization\n",
    "        self.normalize = tf.keras.layers.Normalization()\n",
    "        self.real_init =v0[0] \n",
    "        self.imag_init =v0[1] \n",
    "\n",
    "        if self.init == 0:\n",
    "            kerner_init = tf.keras.initializers.he_normal()\n",
    "        else:\n",
    "            kerner_init = tf.keras.initializers.glorot_normal()\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.initialization = initialization\n",
    "        self.activation = activation\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.hidden_layers = [MyDense(self.units_per_layer, self.initialization, self.activation) \n",
    "                              for _ in range(self.num_of_hid_layers)]\n",
    "        \n",
    "        self.out = MyDense(units=self.num_out, initialization=self.initialization)\n",
    "        \n",
    "        self.bn = [tf.keras.layers.BatchNormalization() for _ in range(self.num_of_hid_layers)]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        Add BN before since the layer includes activations\n",
    "        \n",
    "        '''\n",
    "        for i in range(self.num_of_hid_layers):\n",
    "            if i == 0:\n",
    "                x = self.normalize(inputs)\n",
    "                x = self.bn[i](x)\n",
    "            else:\n",
    "                x = self.bn[i](x)\n",
    "                \n",
    "            x = self.hidden_layers[i](x)\n",
    "        return self.out(x)     \n",
    "    \n",
    "    def pinn_net(self,t):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(t)         \n",
    "            z_real = self.call(t)[:,0:1]\n",
    "            z_imag = self.call(t)[:,1:2]\n",
    "        dz_realdt = tf.cast(tape.gradient(z_real,t),dtype=tf.float32)\n",
    "        dz_imagdt = tf.cast(tape.gradient(z_imag,t),dtype=tf.float32)  \n",
    "        del tape\n",
    "        \n",
    "        \n",
    "#         Residual pde        \n",
    "#     derivatives must be dtype tf.float32\n",
    "#     dzdt = iz(t)\n",
    "        dzdt = tf.complex(dz_realdt,dz_imagdt) # dzdt = dudt + idvdt . \n",
    "        z = tf.complex(z_real,z_imag)\n",
    "        rhs = z*1.j\n",
    "\n",
    "    #     real(LHS)-Real(RHS) = -Imag(LHS)+Img(RHS) Equation from PINN paper\n",
    "\n",
    "        residual_real = tf.math.real(dzdt) - tf.math.real(rhs) # = real(LHS)-Real(RHS)\n",
    "        residual_imag = -tf.math.imag(dzdt) + tf.math.imag(rhs) # = -Imag(LHS)+Img(RHS)\n",
    "        \n",
    "\n",
    "        #         Initial conditions\n",
    "#         residual ics\n",
    "        real0,imag0 = self.call(t[0]*tf.ones_like(t))[:,:1],self.call(t[0]*tf.ones_like(t))[:,1:]\n",
    "#        \n",
    "        residue_ic_real = real0  -  self.real_init\n",
    "        residue_ic_imag = imag0  -  self.imag_init\n",
    "        \n",
    "\n",
    "        return residual_real,residual_imag, residue_ic_real,residue_ic_imag\n",
    "        \n",
    "        \n",
    "    @tf.function    \n",
    "    def loss(self,t):\n",
    "        residual_real,residual_imag, residue_ic_real,residue_ic_imag = self.pinn_net(t)\n",
    "        \n",
    "        loss_real  = tf.reduce_mean(tf.square(residual_real))\n",
    "        loss_imag  = tf.reduce_mean(tf.square(residual_imag))\n",
    "        loss_real_ic = tf.reduce_mean(tf.square(residue_ic_real))\n",
    "        loss_imag_ic = tf.reduce_mean(tf.square(residue_ic_imag))\n",
    "        \n",
    "        return loss_real,loss_real_ic,loss_imag,loss_imag_ic\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        weights_l = []\n",
    "        biases_l = []\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"weight\": self.w,\n",
    "            \"biase\": self.b,\n",
    "            })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "def calc_imbalance(t):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        loss_real,loss_real_ic,loss_imag,loss_imag_ic = z_hat.loss(t)\n",
    "        \n",
    "    dresidue_pde_real = tape.gradient(loss_real,z_hat.trainable_variables)  \n",
    "    dresidue_ic_real  = tape.gradient(loss_real_ic,z_hat.trainable_variables)\n",
    "    \n",
    "    dresidue_pde_imag = tape.gradient(loss_imag,z_hat.trainable_variables)  \n",
    "    dresidue_ic_imag = tape.gradient(loss_imag_ic,z_hat.trainable_variables)\n",
    "    \n",
    "    del tape\n",
    "    return loss_real,loss_real_ic,dresidue_pde_real,dresidue_ic_real, loss_imag,loss_imag_ic,dresidue_pde_imag,dresidue_ic_imag    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a4e7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.cast(tf.linspace(0,1,50)[:,tf.newaxis],tf.float32)\n",
    "real_int = tf.constant(1.0, dtype = tf.float32)\n",
    "imag_int = tf.constant(0.0, dtype = tf.float32)\n",
    "z_init = tf.constant(np.array([real_int,imag_int]),tf.float32)\n",
    "# #initiazing\n",
    "z_hat = Z_approx(z_init,num_of_hid_layers = 3, units_per_layer = 30, num_out=2, initialization = 1,activation = tf.nn.tanh)\n",
    "z_hat.build(t.shape)\n",
    "optim = tf.keras.optimizers.Adam()\n",
    "# real0,imag0 = z_hat(t[0]*tf.ones_like(t))[:,:1],z_hat(t[0]*tf.ones_like(t))[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c221ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_update_jun(t):\n",
    "    S = len(tf.shape_n(z_hat.trainable_variables))\n",
    "    lamda1_real = 1.\n",
    "    lamda1_imag = 1.\n",
    "    \n",
    "    alpha = .9\n",
    "\n",
    "    num_per_layer_real = []\n",
    "    den_per_layer_real = []\n",
    "    \n",
    "    num_per_layer_imag = []\n",
    "    den_per_layer_imag = []  \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_real,loss_real_ic,dresidue_pde_real,dresidue_ic_real, loss_imag,loss_imag_ic,dresidue_pde_imag,dresidue_ic_imag = calc_imbalance(t)\n",
    "#         print(f'loss pde :{residue_pde} loss ic :{residue_ic}')\n",
    "#         print(f'combined loss {residue_pde + residue_ic }')\n",
    "        for i in range(S):\n",
    "        \n",
    "            lamda1_real = 1.\n",
    "            lamda1_imag = 1.\n",
    "            \n",
    "            num_real = tf.reduce_max(tf.abs(dresidue_pde_real[i]))\n",
    "            den_real = tf.reduce_mean(tf.abs(dresidue_ic_real[i]))\n",
    "            \n",
    "            num_imag = tf.reduce_max(tf.abs(dresidue_pde_imag[i]))\n",
    "            den_imag = tf.reduce_mean(tf.abs(dresidue_ic_imag[i]))\n",
    "            \n",
    "\n",
    "            if (den_imag == 0) or (den_real == 0.0):\n",
    "#                 print(f'den_imag = {den_imag}')\n",
    "#                 print(f'den_real = {den_real}')\n",
    "#                 den = 1e-10\n",
    "                \n",
    "            num_per_layer_real.append(num_real)\n",
    "            den_per_layer_real.append(den_real)\n",
    "            \n",
    "            num_per_layer_imag.append(num_imag)\n",
    "            den_per_layer_imag.append(den_imag)\n",
    "\n",
    "        num_final_real =  tf.reduce_max( tf.stack(num_per_layer_real))    #stack flattens list\n",
    "        den_final_real = tf.reduce_mean(tf.stack(den_per_layer_real))\n",
    "        \n",
    "        num_final_imag =  tf.reduce_max( tf.stack(num_per_layer_imag))    #stack flattens list\n",
    "        den_final_imag = tf.reduce_mean(tf.stack(den_per_layer_imag))        \n",
    "\n",
    "        lambda_hat_real = num_final_real/den_final_real\n",
    "        lambda_hat_imag = num_final_imag/den_final_imag\n",
    "\n",
    "        lamda1_real = (1.-alpha)*lamda1_real + alpha*lambda_hat_real\n",
    "        lamda1_imag = (1.-alpha)*lamda1_imag + alpha*lambda_hat_imag\n",
    "\n",
    "#         print(f'lambda for layer {i} is {lamda1}')\n",
    "#         print(f'loss pde :{residue_pde} loss ic :{residue_ic}')\n",
    "#         print(f'loss pde :{residue_pde} loss balcanced :{lamda1*residue_ic}')\n",
    "\n",
    "\n",
    "        new_loss_real = loss_real + lamda1_real*loss_real_ic\n",
    "#         print(f'Real loss :{new_loss_real}')\n",
    "        new_loss_imag = loss_imag + lamda1_imag*loss_imag_ic\n",
    "#         print(f'Imag loss :{new_loss_imag}')\n",
    "        new_loss = new_loss_real + new_loss_imag\n",
    "#         print(f'final loss before :{new_loss}')\n",
    "    dddd = tape.gradient(new_loss, z_hat.trainable_variables)    \n",
    "    optim.apply_gradients(zip(dddd, z_hat.trainable_variables))\n",
    "    return new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac56fc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1<2 or 2>3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8484e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "den_imag = Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_3:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_5:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_4:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_7:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_6:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_9:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_8:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_11:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_10:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_13:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_12:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_15:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_14:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_17:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_16:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_19:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_18:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_21:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_20:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_23:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_22:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_25:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_24:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_27:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_26:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_3:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_5:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_4:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_7:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_6:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_9:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_8:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_11:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_10:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_13:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_12:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_15:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_14:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_17:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_16:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_19:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_18:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_21:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_20:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_23:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_22:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_25:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_24:0\", shape=(), dtype=float32)\n",
      "den_imag = Tensor(\"Mean_27:0\", shape=(), dtype=float32)\n",
      "den_real = Tensor(\"Mean_26:0\", shape=(), dtype=float32)\n",
      "loss for epoch 0 : 2.459538459777832 best so far 2.459538459777832\n",
      ".loss for epoch 1000 : 0.07192030549049377 best so far 0.07192030549049377\n",
      ".loss for epoch 2000 : 0.018223704770207405 best so far 0.014727523550391197\n",
      ".loss for epoch 3000 : 0.0008695921860635281 best so far 0.0008693558629602194\n",
      ".loss for epoch 4000 : 0.00010649549949448556 best so far 0.00010210687469225377\n",
      ".loss for epoch 5000 : 9.925916856445838e-06 best so far 9.25417771213688e-06\n",
      ".Loss :nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f905f018130>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "losses = []\n",
    "loss_decrease = []\n",
    "best_so_far = np.inf\n",
    "losses.append(train_update_jun(t))\n",
    "\n",
    "for epoch in range(50000):\n",
    "    loss = train_update_jun(t)\n",
    "    if loss < losses[-1]:\n",
    "        if loss < best_so_far:\n",
    "            best_so_far = loss\n",
    "            loss_decrease.append(loss)\n",
    "#                 print(f'******************best so far is updated to :{best_so_far}********************')\n",
    "            z_hat.save_weights('best_model')\n",
    "             \n",
    "\n",
    "    if np.isnan(loss):\n",
    "        print(f'Loss :{loss}')\n",
    "        break\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    if epoch%1000==0:\n",
    "        print(f'loss for epoch {epoch} : {loss} best so far {best_so_far}') \n",
    "        print(end = \".\")\n",
    "\n",
    "z_hat.load_weights('best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e33d79af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=6.20706e-07>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.5765078e-07>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=7.223204e-06>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.1338132e-08>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_hat.loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a610080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f905f19e7c0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# True solutions\n",
    "x_true = lambda t : tf.math.cos(t)\n",
    "y_true = lambda t : tf.math.sin(t)\n",
    "\n",
    "plt.plot(t, x_true(t) ,label = 'real')\n",
    "plt.plot(t,  y_true(t), label = 'imag')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "480a00bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f905deb2340>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t, x_true(t) ,label = 'Real_soln')\n",
    "plt.plot(t, z_hat(t)[:,0:1], label = 'NN_soln')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(t, y_true(t) ,label = 'Imag_soln')\n",
    "plt.plot(t, z_hat(t)[:,1:], label = 'NN_soln')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4d09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ba5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b51c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_optimizers():\n",
    "    shapes = tf.shape_n(z_hat.trainable_variables)\n",
    "    length_shapes = len(shapes)\n",
    "    count = 0\n",
    "    idx = []\n",
    "    part = []\n",
    "\n",
    "#     loss_pde,loss_ic = train_update_jun(t)\n",
    "    total_loss = train_update_jun(t)\n",
    "    print(f'Initial loss : {total_loss}')\n",
    "\n",
    "\n",
    "    for i in range(length_shapes):\n",
    "        n = np.product(shapes[i])\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "        count += n   \n",
    "        part.extend([i]*n)\n",
    "\n",
    "    def update_weights(OneD):\n",
    "        params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            z_hat.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    def flatten_the_parameters(weights):\n",
    "        return tf.dynamic_stitch(idx, weights) \n",
    "\n",
    "#     @tf.function\n",
    "    def value_and_gradient(flat_weights):\n",
    "        #update weights \n",
    "        update_weights(flat_weights)\n",
    "        with tf.GradientTape() as g:\n",
    "            los = train_update_jun(t)\n",
    "        grad = g.gradient(los, z_hat.trainable_variables) \n",
    "        flattend_grad = flatten_the_parameters(grad) #converts from multDim to 1D\n",
    "        return los, flattend_grad  \n",
    "\n",
    "\n",
    "    start_time2 = time.time()\n",
    "\n",
    "    flat_parameters = flatten_the_parameters(z_hat.trainable_variables)\n",
    "    start = tf.constant(flat_parameters)  # Starting point for the search.\n",
    "    grads_lists = []\n",
    "    optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "      value_and_gradient, initial_position=start, tolerance=1.0e-20, max_iterations=5000)\n",
    "    \n",
    "    if optim_results.\n",
    "    grad_curr = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "    grads_lists.append(grad_curr)\n",
    "    \n",
    "#     if optim_results.converged.numpy() == True:\n",
    "#         break\n",
    "\n",
    "    while  optim_results.converged.numpy() == False:\n",
    "        optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "      value_and_gradient, initial_position=start, max_iterations=5000)\n",
    "        print('converged:', optim_results.converged.numpy())\n",
    "        print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "        print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "        print('')\n",
    "        new_grad = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "\n",
    "        if new_grad == grads_lists[-1]:\n",
    "            print('NO change')\n",
    "            loss = train_update_jun(t)\n",
    "            if loss < 1e-6:\n",
    "                print('satiesfied')\n",
    "                break\n",
    "            else:\n",
    "                mix_optimizers()\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    print('converged:', optim_results.converged.numpy())\n",
    "    print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "    print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "    update_weights(optim_results.position)\n",
    "\n",
    "    loss_pde,loss_ic = z_hat.loss(t)\n",
    "    los = loss_pde + loss_ic\n",
    "    print('loss',los)\n",
    "    print('Times : ',time.time()-start_time2,'\\n')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4371fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39902bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = length_shapes\n",
    "lamda1 = 1.\n",
    "\n",
    "num_per_layer = []\n",
    "den_per_layer = []\n",
    "with tf.GradientTape() as tape:\n",
    "    residue_pde, residue_ic,dresidue_pde,dresidue_ic   = calc_imbalance(t)\n",
    "    print(f'loss pde :{residue_pde} loss ic :{residue_ic}')\n",
    "    print(f'combined loss {residue_pde + residue_ic }')\n",
    "    for i in range(S):\n",
    "        lamda1 = 1.\n",
    "        num = tf.reduce_max(tf.abs(tf.reshape(dresidue_pde[i],(-1))))\n",
    "        den = tf.reduce_mean(tf.abs(tf.reshape(dresidue_ic[i],(-1))))\n",
    "\n",
    "        num_per_layer.append(num)\n",
    "        den_per_layer.append(den)\n",
    "\n",
    "    num_final =  tf.reduce_max( tf.stack(num_per_layer))    #stack flattens list\n",
    "    den_final = tf.reduce_mean(tf.stack(den_per_layer))\n",
    "\n",
    "    lambda_hat = num_final/den_final\n",
    "\n",
    "    lamda1 = (1.-alpha)*lamda1 + alpha*lambda_hat\n",
    "\n",
    "    print(f'lambda for layer {i} is {lamda1}')\n",
    "    print(f'loss pde :{residue_pde} loss ic :{residue_ic}')\n",
    "    print(f'loss pde :{residue_pde} loss balcanced :{lamda1*residue_ic}')\n",
    "    \n",
    "\n",
    "    new_loss = residue_pde + lamda1*residue_ic\n",
    "    print(f'final loss before :{new_loss}')\n",
    "dddd = tape.gradient(new_loss, z_hat.trainable_variables)    \n",
    "optim.apply_gradients(zip(dddd, z_hat.trainable_variables))\n",
    "\n",
    "residue_pde, residue_ic,dresidue_pde,dresidue_ic   = calc_imbalance(t)\n",
    "new_loss = residue_pde + lamda1*residue_ic\n",
    "new_loss = residue_pde + lamda1*residue_ic\n",
    "print(f'final loss after update :{new_loss}')\n",
    "#     plt.title('Gradients for parameter '+str(i))\n",
    "#     plt.plot(tf.reshape(dresidue_pde[i],(-1)), label ='pde_res'+str(i) )\n",
    "#     plt.plot(tf.reshape(dresidue_ic[i],(-1)),label ='ics_res'+str(i))\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.title('modified Gradients for parameter '+str(i))\n",
    "#     plt.plot(tf.reshape(dresidue_pde[i],(-1)), label ='mod_pde_res'+str(i) )\n",
    "#     plt.plot(tf.reshape(lamda1*dresidue_ic[i],(-1)),label ='mod_ics_res'+str(i))\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#             if tf.size(grad_residual_x[n].shape) == 2 and grad_residual_x[n].shape[0] == 1:\n",
    "#                 plt.plot(grad_residual_x[n][0],label = 'grad_res')\n",
    "#                 plt.plot(grad_mse_x0_real[n][0],label = 'grad_IC')\n",
    "#                 plt.title('layer_corrected = '+str(n))\n",
    "#                 plt.legend()\n",
    "#                 plt.show()\n",
    "\n",
    "\n",
    "#             print(f'shapes:{grad_residual_x[n].shape} \\n reduce_prod :{tf.reduce_prod(grad_residual_x[n].shape)}')\n",
    "#     max_grad_res_list_real.append( tf.reduce_max(tf.math.abs(grad_residual_x[n])))\n",
    "#     max_grad_res_list_imag.append( tf.reduce_max(tf.math.abs(grad_residual_y[n])))\n",
    "\n",
    "#     max_grad_res_list_real_bc.append( tf.reduce_mean(tf.math.abs(grad_mse_x0_real[n])))\n",
    "#     max_grad_res_list_imag_bc.append( tf.reduce_mean(tf.math.abs(grad_mse_y0_imag[n])) )\n",
    "\n",
    "# lambda_residue_hat_real = tf.reduce_max(max_grad_res_list_real) / tf.reduce_mean(max_grad_res_list_real_bc)\n",
    "# lambda_residue_hat_imag = tf.reduce_max(max_grad_res_list_imag) / tf.reduce_mean(max_grad_res_list_imag_bc)\n",
    "\n",
    "# lambda_real = (1. - alpha)*lambda_real + alpha*lambda_residue_hat_real\n",
    "# lambda_imag = (1. - alpha)*lambda_imag + alpha*lambda_residue_hat_imag \n",
    "\n",
    "# total_loss_updated = residual_x + mse_x0_real*lambda_real  + residual_y + mse_y0_imag*lambda_imag\n",
    "# #         print(f'loss residue_real :{residual_x} ,loss residue_imag :{residual_y}' )\n",
    "# #         print('Old loss_bc')\n",
    "# #         print(f'loss residue_real_bc :{mse_x0_real} ,loss residue_imag_bc :{mse_y0_imag}' )\n",
    "# #         print('New loss_bc')\n",
    "# #         print(f'loss residue_real_bc :{mse_x0_real*lambda_real} ,loss residue_imag_bc :{mse_y0_imag*lambda_imag}' )        \n",
    "# #         print(f'New lambda_real :{lambda_real}...New lambda_imag :{lambda_imag} ')\n",
    "# return total_loss_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11262fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(tf.abs(flatten_the_parameters(dresidue_ic,idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_loss(t) :\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def to_fix(z_hat):\n",
    "        shapes = tf.shape_n(z_hat.trainable_variables)\n",
    "        length_shapes = len(shapes)\n",
    "        count = 0\n",
    "        idx = []\n",
    "        part = tf.TensorArray(tf.int32, size=0, dynamic_size=True, clear_after_read=False,infer_shape=False)\n",
    "        # part = []\n",
    "\n",
    "    #     loss_pde,loss_ic = z_hat.loss(t)\n",
    "    #     total_loss = loss_pde + loss_ic\n",
    "\n",
    "        for i in range(length_shapes):\n",
    "            n = tf.reduce_prod(shapes[i])\n",
    "            idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "            count += n  \n",
    "            part = part.write(i,i*tf.ones(n,tf.int32))\n",
    "    #         print(part.read(i)   )\n",
    "    \n",
    "        def tensor_to_list(tensor):\n",
    "            length_shapes = len(tf.shape_n(z_hat.trainable_variables))\n",
    "            part_list = []\n",
    "            for i in range(length_shapes):\n",
    "                part_list.extend(part.read(i).numpy())\n",
    "            return part_list \n",
    "        \n",
    "        return idx,tensor_to_list(part)\n",
    "\n",
    "\n",
    "    def update_weights(OneD,part):\n",
    "        params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            z_hat.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    def flatten_the_parameters(weights,idx):\n",
    "        return tf.dynamic_stitch(idx, weights) \n",
    "\n",
    "    weights = z_hat.trainable_variables\n",
    "    idx, part = to_fix(z_hat)\n",
    "    \n",
    "    dresidue_pde,dresidue_ic   =  calc_imbalance(t)\n",
    "    num = tf.reduce_max(tf.abs(flatten_the_parameters(dresidue_pde,idx)))\n",
    "    den = tf.reduce_mean(tf.abs(flatten_the_parameters(dresidue_ic,idx)))\n",
    "    lambda_hat = num/den\n",
    "    residue_pde, residue_ic = z_hat.loss(t)\n",
    "    return residue_pde +lamda1*residue_ic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb663e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = z_hat.trainable_variables\n",
    "idx, part = to_fix(z_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ede71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dresidue_pde,dresidue_ic   =  calc_imbalance(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_loss(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4080a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_update_with_balanced_loss(t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        bloss = balance_loss(t)\n",
    "    dbloss_up = tape.gradient(bloss,z_hat.trainable_variables)  \n",
    "    optim.apply_gradients(zip(dbloss_up, z_hat.trainable_variables))\n",
    "    return bloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e4ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "losses =[]\n",
    "loss_decrease = []\n",
    "check_decrease = 0\n",
    "initial_loss = train_and_update_with_balanced_loss(t).numpy()\n",
    "print(f'Initial loss :{initial_loss}')\n",
    "best_so_far =  np.inf\n",
    "losses.append(initial_loss)\n",
    "for epoch in tnrange(epochs):\n",
    "    loss = train_and_update_with_balanced_loss(t).numpy()\n",
    "    if loss < losses[-1]:\n",
    "        if loss < best_so_far:\n",
    "            best_so_far = loss\n",
    "            loss_decrease.append(loss)\n",
    "#                 print(f'******************best so far is updated to :{best_so_far}********************')\n",
    "            z_hat.save_weights('best_model')\n",
    "            check_decrease = loss  \n",
    "    losses.append(loss)\n",
    "\n",
    "    if epoch%1000==0:\n",
    "        print(f'loss for epoch {epoch} : {loss} best so far {best_so_far}') \n",
    "        print(end = \".\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_optimizers():\n",
    "    shapes = tf.shape_n(z_hat.trainable_variables)\n",
    "    length_shapes = len(shapes)\n",
    "    count = 0\n",
    "    idx = []\n",
    "    part = []\n",
    "\n",
    "    loss_pde,loss_ic = z_hat.loss(t)\n",
    "    total_loss = train_and_update_with_balanced_loss(t)\n",
    "    print(f'Initial loss : {total_loss}')\n",
    "\n",
    "\n",
    "    for i in range(length_shapes):\n",
    "        n = np.product(shapes[i])\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "        count += n   \n",
    "        part.extend([i]*n)\n",
    "\n",
    "    def update_weights(OneD):\n",
    "        params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            z_hat.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    def flatten_the_parameters(weights):\n",
    "        return tf.dynamic_stitch(idx, weights) \n",
    "\n",
    "#     @tf.function\n",
    "    def value_and_gradient(flat_weights):\n",
    "        #update weights \n",
    "        update_weights(flat_weights)\n",
    "        with tf.GradientTape() as g:\n",
    "            loss_pde,loss_ic = z_hat.loss(t)\n",
    "            los = train_and_update_with_balanced_loss(t)\n",
    "        grad = g.gradient(los, z_hat.trainable_variables) \n",
    "        flattend_grad = flatten_the_parameters(grad) #converts from multDim to 1D\n",
    "        return los, flattend_grad  \n",
    "\n",
    "\n",
    "    start_time2 = time.time()\n",
    "\n",
    "    flat_parameters = flatten_the_parameters(z_hat.trainable_variables)\n",
    "    start = tf.constant(flat_parameters)  # Starting point for the search.\n",
    "    grads_lists = []\n",
    "    optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "      value_and_gradient, initial_position=start, tolerance=1.0e-20, max_iterations=5000)\n",
    "    grad_curr = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "    grads_lists.append(grad_curr)\n",
    "    \n",
    "    while  optim_results.converged.numpy() == False:\n",
    "        optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "      value_and_gradient, initial_position=start, max_iterations=5000)\n",
    "        print('converged:', optim_results.converged.numpy())\n",
    "        print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "        print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "        print('')\n",
    "        new_grad = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "\n",
    "        if new_grad == grads_lists[-1]:\n",
    "            print('NO change')\n",
    "#             train_update_step(t,10000,True)\n",
    "#             mix_optimizers()\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    print('converged:', optim_results.converged.numpy())\n",
    "    print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "    print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "    update_weights(optim_results.position)\n",
    "\n",
    "    loss_pde,loss_ic = z_hat.loss(t)\n",
    "    los = loss_pde + loss_ic\n",
    "    print('loss',los)\n",
    "    print('Times : ',time.time()-start_time2,'\\n')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f727f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for epoch in tnrange(epochs):\n",
    "        loss = train_and_update(t).numpy()\n",
    "\n",
    "        if loss < losses[-1]:\n",
    "            if loss < best_so_far:\n",
    "                best_so_far = loss\n",
    "                loss_decrease.append(loss)\n",
    "#                 print(f'******************best so far is updated to :{best_so_far}********************')\n",
    "                z_hat.save_weights('best_model')\n",
    "                check_decrease = loss  \n",
    "\n",
    "        if np.isnan(loss):\n",
    "            print(f'Loss :{loss}')\n",
    "            break\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        if epoch%1000==0:\n",
    "            print(f'loss for epoch {epoch} : {loss} best so far {best_so_far}') \n",
    "            print(end = \".\")\n",
    "\n",
    "    z_hat.load_weights('best_model')\n",
    "    \n",
    "    mix_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b201b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5098844",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_update_with_balanced_loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447201a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dresidue_pde,dresidue_ic = calc_imbalance(t)\n",
    "lamda1 = 1.\n",
    "alpha = 0.9\n",
    "\n",
    "for i in range(len(tf.shape_n(z_hat.trainable_variables))):\n",
    "    num = tf.reduce_max(tf.abs(tf.reshape(dresidue_pde[i],(-1))))\n",
    "    den = tf.reduce_mean(tf.abs(tf.reshape(dresidue_ic[i],(-1))))+1e-10\n",
    "    \n",
    "    lambda_hat = num/den\n",
    "    \n",
    "    lamda1 = (1.-alpha)*lamda1 + alpha*lambda_hat\n",
    "        \n",
    "    print(f'lambda hat for layer {i} is {lamda1}')\n",
    "    print(f'dres ic {tf.reshape(dresidue_ic[i],(-1))[0]}')\n",
    "#     print(tf.reshape(dresidue_pde[i],(-1)), tf.reshape(lamda1*dresidue_ic[i],(-1)))\n",
    "    \n",
    "    \n",
    "    plt.title('Gradients for parameter '+str(i))\n",
    "    plt.plot(tf.reshape(dresidue_pde[i],(-1)), label ='pde_res'+str(i) )\n",
    "    plt.plot(tf.reshape(dresidue_ic[i],(-1)),label ='ics_res'+str(i))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title('modified Gradients for parameter '+str(i))\n",
    "    plt.plot(tf.reshape(dresidue_pde[i],(-1)), label ='mod_pde_res'+str(i) )\n",
    "    plt.plot(tf.reshape(lamda1*dresidue_ic[i],(-1)),label ='mod_ics_res'+str(i))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448530de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980fb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479f990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11988b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_and_update(t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_pde,loss_ic = z_hat.loss(t)\n",
    "        total_loss = loss_pde + loss_ic\n",
    "    dlossdpa = tape.gradient(total_loss, z_hat.trainable_variables) \n",
    "    optim.apply_gradients(zip(dlossdpa, z_hat.trainable_variables))\n",
    "    return total_loss\n",
    "\n",
    "def calc_imbalance(t):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        residue_pde, residue_ic = z_hat.loss(t)\n",
    "    dresidue_pde = tape.gradient(residue_pde,z_hat.trainable_variables)  \n",
    "    dresidue_ic = tape.gradient(residue_ic,z_hat.trainable_variables)\n",
    "    \n",
    "    del tape\n",
    "    return dresidue_pde,dresidue_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dresidue_pde,dresidue_ic = calc_imbalance(t)\n",
    "for i in range(len(tf.shape_n(z_hat.trainable_variables))):\n",
    "    plt.title('Gradients for parameter '+str(i))\n",
    "    plt.plot(tf.reshape(dresidue_pde[i],(-1)), label ='pde_res'+str(i) )\n",
    "    plt.plot(tf.reshape(dresidue_ic[i],(-1)),label ='ics_res'+str(i))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42867d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(tf.reshape(dresidue_pde[0],(-1)), kind='kde',label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "sns.displot(tf.reshape(dresidue_ic[0],(-1)), kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_optimizers():\n",
    "    shapes = tf.shape_n(z_hat.trainable_variables)\n",
    "    length_shapes = len(shapes)\n",
    "    count = 0\n",
    "    idx = []\n",
    "    part = []\n",
    "\n",
    "    loss_pde,loss_ic = z_hat.loss(t)\n",
    "    total_loss = loss_pde + loss_ic\n",
    "    print(f'Initial loss : {total_loss}')\n",
    "\n",
    "\n",
    "    for i in range(length_shapes):\n",
    "        n = np.product(shapes[i])\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "        count += n   \n",
    "        part.extend([i]*n)\n",
    "\n",
    "    def update_weights(OneD):\n",
    "        params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            z_hat.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    def flatten_the_parameters(weights):\n",
    "        return tf.dynamic_stitch(idx, weights) \n",
    "\n",
    "    @tf.function\n",
    "    def value_and_gradient(flat_weights):\n",
    "        #update weights \n",
    "        update_weights(flat_weights)\n",
    "        with tf.GradientTape() as g:\n",
    "            loss_pde,loss_ic = z_hat.loss(t)\n",
    "            los = loss_pde + loss_ic\n",
    "        grad = g.gradient(los, z_hat.trainable_variables) \n",
    "        flattend_grad = flatten_the_parameters(grad) #converts from multDim to 1D\n",
    "        return los, flattend_grad  \n",
    "\n",
    "\n",
    "    start_time2 = time.time()\n",
    "\n",
    "    flat_parameters = flatten_the_parameters(z_hat.trainable_variables)\n",
    "    start = tf.constant(flat_parameters)  # Starting point for the search.\n",
    "    grads_lists = []\n",
    "    optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "      value_and_gradient, initial_position=start, tolerance=1.0e-20, max_iterations=5000)\n",
    "    grad_curr = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "    grads_lists.append(grad_curr)\n",
    "\n",
    "    while  optim_results.converged.numpy() == False:\n",
    "        optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "      value_and_gradient, initial_position=start, max_iterations=5000)\n",
    "        print('converged:', optim_results.converged.numpy())\n",
    "        print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "        print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "        print('')\n",
    "        new_grad = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "\n",
    "        if new_grad == grads_lists[-1]:\n",
    "            print('NO change')\n",
    "#             train_update_step(t,10000,True)\n",
    "#             mix_optimizers()\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    print('converged:', optim_results.converged.numpy())\n",
    "    print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "    print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "    update_weights(optim_results.position)\n",
    "\n",
    "    loss_pde,loss_ic = z_hat.loss(t)\n",
    "    los = loss_pde + loss_ic\n",
    "    print('loss',los)\n",
    "    print('Times : ',time.time()-start_time2,'\\n')      \n",
    "\n",
    "\n",
    "def train_update_step(t,epochs,rerun):\n",
    "    losses =[]\n",
    "    loss_decrease = []\n",
    "    check_decrease = 0\n",
    "    initial_loss = train_and_update(t).numpy()\n",
    "    if rerun:\n",
    "        best_so_far = initial_loss\n",
    "    else:\n",
    "        best_so_far =  np.inf\n",
    "    \n",
    "    \n",
    "    \n",
    "    best_so_far =  np.inf\n",
    "    print(f'initial loss : {initial_loss}')\n",
    "    losses.append(initial_loss)\n",
    "    for epoch in tnrange(epochs):\n",
    "        loss = train_and_update(t).numpy()\n",
    "\n",
    "        if loss < losses[-1]:\n",
    "            if loss < best_so_far:\n",
    "                best_so_far = loss\n",
    "                loss_decrease.append(loss)\n",
    "#                 print(f'******************best so far is updated to :{best_so_far}********************')\n",
    "                z_hat.save_weights('best_model')\n",
    "                check_decrease = loss  \n",
    "\n",
    "        if np.isnan(loss):\n",
    "            print(f'Loss :{loss}')\n",
    "            break\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        if epoch%1000==0:\n",
    "            print(f'loss for epoch {epoch} : {loss} best so far {best_so_far}') \n",
    "            print(end = \".\")\n",
    "\n",
    "    z_hat.load_weights('best_model')\n",
    "    \n",
    "    mix_optimizers()\n",
    "\n",
    "    return losses,loss_decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99adc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses,loss_decrease = train_update_step(t,10000,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(losses)\n",
    "plt.plot(loss_decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daea027",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_v = lambda t: g*t + v0\n",
    "plt.plot(t,true_v(t),'o')\n",
    "plt.plot(t, z_hat(t))\n",
    "plt.legend(['True','NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(tf.abs(tf.reshape(dresidue_pde[0],(-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16113b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(tf.abs(tf.reshape(dresidue_ic[0],(-1))))+1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dresidue_pde,dresidue_ic = calc_imbalance(t)\n",
    "lamda1 = 1.\n",
    "alpha = 1.\n",
    "\n",
    "for i in range(len(tf.shape_n(z_hat.trainable_variables))):\n",
    "    num = tf.reduce_max(tf.abs(tf.reshape(dresidue_pde[i],(-1))))\n",
    "    den = tf.reduce_mean(tf.abs(tf.reshape(dresidue_ic[i],(-1))))+1e-10\n",
    "    \n",
    "    lambda_hat = num/den\n",
    "    \n",
    "    lamda1 = (1.-alpha)*lamda1 + alpha*lambda_hat\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.title('Gradients for parameter '+str(i))\n",
    "    plt.plot(tf.reshape(dresidue_pde[i],(-1)), label ='pde_res'+str(i) )\n",
    "    plt.plot(tf.reshape(dresidue_ic[i],(-1)),label ='ics_res'+str(i))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title('modified Gradients for parameter '+str(i))\n",
    "    plt.plot(tf.reshape(dresidue_pde[i],(-1)), label ='mod_pde_res'+str(i) )\n",
    "    plt.plot(tf.reshape(lamda1*dresidue_ic[i],(-1)),label ='mod_ics_res'+str(i))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1780470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549f339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2c8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e61a276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7444ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003f267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd33b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_update(t).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8077da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses,loss_decrease = train_whole_set(t,1000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = 00\n",
    "\n",
    "if ll: \n",
    "    print('Yes')\n",
    "else:\n",
    "    print('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13342f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4935845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape_n(z_hat.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d38dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    def __init__(self, t, model,z_initm,optim):\n",
    "        self.model = model\n",
    "        self.t = t\n",
    "        self.x_init = z_init[0]\n",
    "        self.y_init = z_init[1]\n",
    "        self.optim = optim\n",
    "\n",
    "        \n",
    "    def function_and_derivatives(self,t):\n",
    "        '''\n",
    "        Calculate the derivatives and function that correspond to the ode\n",
    "\n",
    "        input:\n",
    "                t : independent variant\n",
    "\n",
    "        output:\n",
    "                value and corresponding derivative\n",
    "\n",
    "        '''\n",
    "        \n",
    "#         z = u + iv\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(t)         \n",
    "            v = self.model(t)\n",
    "        dvdt = tf.cast(tape.gradient(v,t),dtype=tf.float32)  \n",
    "        return u,dvdt\n",
    "    \n",
    "    def equations(self, u,v,dudt,dvdt):\n",
    "    #     derivatives must be dtype tf.float32\n",
    "    #     dzdt = iz(t)\n",
    "        dzdt = tf.complex(dudt,dvdt) # dzdt = dudt + idvdt . \n",
    "        z = tf.complex(u,v)\n",
    "        rhs = z*1.j\n",
    "\n",
    "    #     real(LHS)-Real(RHS) = -Imag(LHS)+Img(RHS) Equation from PINN paper\n",
    "\n",
    "        f_real = tf.math.real(dzdt) - tf.math.real(rhs) # = real(LHS)-Real(RHS)\n",
    "        f_imag = -tf.math.imag(dzdt) + tf.math.imag(rhs) # = -Imag(LHS)+Img(RHS)\n",
    "        return f_real,f_imag\n",
    "        \n",
    "    def ode_ic(self,t):\n",
    "        u,v, dudt,dvdt  = self.get_derivatives(t)\n",
    "        f_real,f_imag = self.equations(u,v, dudt,dvdt)\n",
    "        u1 =  f_real   #residual of real numbers\n",
    "        u2 =  f_imag   #residual of Imag numbers\n",
    "#         Initial conditions\n",
    "        u0,v0,_,_ = self.get_derivatives(10**(-10)*tf.ones_like(t))\n",
    "#        \n",
    "        u3 = u0 - self.x_init\n",
    "        u4 = v0  - self.y_init\n",
    "\n",
    "        return u1,u2,u3,u4\n",
    "        \n",
    "    def loss(self,t):\n",
    "        u1,u2,u3,u4 = self.ode_ic(t)\n",
    "        residual_x  = tf.reduce_mean(tf.square(u1))\n",
    "        residual_y  = tf.reduce_mean(tf.square(u2))\n",
    "        mse_x0_real = tf.reduce_mean(tf.square(u3))\n",
    "        mse_y0_imag = tf.reduce_mean(tf.square(u4))\n",
    "#         print(f'mse_x :{residual_x}, mse_y : {residual_y}')\n",
    "#         print('Same order as bc')\n",
    "#         print(f'mse_x0 :{mse_x0_real}, mse_y0 : {mse_y0_imag}')\n",
    "        return residual_x,residual_y,mse_x0_real,mse_y0_imag\n",
    "    \n",
    "    def lambda_hat(self, grad_residue_ode, grad_residue_ic):\n",
    "        numerator   = tf.math.reduce_max(tf.math.abs(grad_residue_ode))\n",
    "        denominator = tf.reduce_mean(tf.math.abs(grad_residue_ic))\n",
    "        return numerator/denominator\n",
    "        \n",
    "    def annealing_update(self,t):    \n",
    "        alpha = tf.constant(0.9, tf.float32)\n",
    "        eta = tf.constant(10**(-3), tf.float32)\n",
    "        \n",
    "        lambda_real = tf.constant(1., tf.float32)\n",
    "        lambda_imag = tf.constant(1., tf.float32)\n",
    "        \n",
    "               \n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            residual_x,residual_y,mse_x0_real,mse_y0_imag = self.loss(t)\n",
    "            total_loss = residual_x + mse_x0_real*lambda_real +   residual_y + mse_y0_imag*lambda_imag \n",
    "#             print('first loss',new_loss)\n",
    "            \n",
    "        grad_residual_x = tape.gradient(residual_x,self.model.trainable_variables)  \n",
    "        grad_residual_y = tape.gradient(residual_y,self.model.trainable_variables)\n",
    "        \n",
    "        grad_mse_x0_real = tape.gradient(mse_x0_real,self.model.trainable_variables)\n",
    "        grad_mse_y0_imag = tape.gradient(mse_y0_imag,self.model.trainable_variables)\n",
    "        del tape\n",
    "        \n",
    "#         return grad_residual_x,grad_residual_y,grad_mse_x0_real,grad_mse_y0_imag\n",
    "        \n",
    "\n",
    "        \n",
    "        S = len(tf.shape_n(self.model.trainable_variables))\n",
    "        \n",
    "            \n",
    "        max_grad_res_list_real = []\n",
    "        max_grad_res_list_imag = []\n",
    "        max_grad_res_list_real_bc =[]\n",
    "        max_grad_res_list_imag_bc = []\n",
    "        \n",
    "        for n in range(S):\n",
    "#             if tf.size(grad_residual_x[n].shape) == 2 and grad_residual_x[n].shape[0] == 1:\n",
    "#                 plt.plot(grad_residual_x[n][0],label = 'grad_res')\n",
    "#                 plt.plot(grad_mse_x0_real[n][0],label = 'grad_IC')\n",
    "#                 plt.title('layer_corrected = '+str(n))\n",
    "#                 plt.legend()\n",
    "#                 plt.show()\n",
    "            \n",
    "#             print(f'shapes:{grad_residual_x[n].shape} \\n reduce_prod :{tf.reduce_prod(grad_residual_x[n].shape)}')\n",
    "            max_grad_res_list_real.append( tf.reduce_max(tf.math.abs(grad_residual_x[n])))\n",
    "            max_grad_res_list_imag.append( tf.reduce_max(tf.math.abs(grad_residual_y[n])))\n",
    "\n",
    "            max_grad_res_list_real_bc.append( tf.reduce_mean(tf.math.abs(grad_mse_x0_real[n])))\n",
    "            max_grad_res_list_imag_bc.append( tf.reduce_mean(tf.math.abs(grad_mse_y0_imag[n])) )\n",
    "            \n",
    "        lambda_residue_hat_real = tf.reduce_max(max_grad_res_list_real) / tf.reduce_mean(max_grad_res_list_real_bc)\n",
    "        lambda_residue_hat_imag = tf.reduce_max(max_grad_res_list_imag) / tf.reduce_mean(max_grad_res_list_imag_bc)\n",
    "        \n",
    "        lambda_real = (1. - alpha)*lambda_real + alpha*lambda_residue_hat_real\n",
    "        lambda_imag = (1. - alpha)*lambda_imag + alpha*lambda_residue_hat_imag \n",
    "\n",
    "        total_loss_updated = residual_x + mse_x0_real*lambda_real  + residual_y + mse_y0_imag*lambda_imag\n",
    "#         print(f'loss residue_real :{residual_x} ,loss residue_imag :{residual_y}' )\n",
    "#         print('Old loss_bc')\n",
    "#         print(f'loss residue_real_bc :{mse_x0_real} ,loss residue_imag_bc :{mse_y0_imag}' )\n",
    "#         print('New loss_bc')\n",
    "#         print(f'loss residue_real_bc :{mse_x0_real*lambda_real} ,loss residue_imag_bc :{mse_y0_imag*lambda_imag}' )        \n",
    "#         print(f'New lambda_real :{lambda_real}...New lambda_imag :{lambda_imag} ')\n",
    "        return total_loss_updated\n",
    "\n",
    "    @tf.function\n",
    "    def train_and_update(self,t):\n",
    "        with tf.GradientTape() as tape:\n",
    "            los  = self.annealing_update(t)\n",
    "        dlossdpa = tape.gradient(los,self.model.trainable_variables) \n",
    "        self.optim.apply_gradients(zip(dlossdpa, self.model.trainable_variables))\n",
    "        return los\n",
    "\n",
    "    def train_whole_set(self, t,epochs, rerun):     \n",
    "        losses =[]\n",
    "        loss_decrease = []\n",
    "        check_decrease = 0\n",
    "        initial_loss = self.train_and_update(t).numpy()\n",
    "        if rerun:\n",
    "            best_so_far = initial_loss\n",
    "        else:\n",
    "            best_so_far =  np.inf\n",
    "        print(f'initial loss : {initial_loss}')\n",
    "        losses.append(initial_loss)\n",
    "\n",
    "        for epoch in tnrange(epochs):\n",
    "            loss = self.train_and_update(t).numpy()\n",
    "\n",
    "            if loss < losses[-1]:\n",
    "                if loss < best_so_far:\n",
    "                    best_so_far = loss\n",
    "                    loss_decrease.append(loss)\n",
    "                    self.model.save_weights('best_model')\n",
    "                    check_decrease = loss\n",
    "\n",
    "            \n",
    "\n",
    "            if np.isnan(loss):\n",
    "                print(f'Loss :{loss}')\n",
    "                break\n",
    "            \n",
    "            losses.append(loss)\n",
    "\n",
    "            if epoch%500==0:\n",
    "                print(f'loss for epoch {epoch} : {loss} best so far {best_so_far}') \n",
    "        #         print(end = \".\")\n",
    "\n",
    "        self.model.load_weights('best_model')\n",
    "        \n",
    "\n",
    "#             print(f'Final loss : {loss}  best so far {best_so_far}') \n",
    "#             print(f'Last decrease : {check_decrease}')\n",
    "        return losses,loss_decrease\n",
    "\n",
    "\n",
    "    def train_lbfgs(self,model,loss,t):\n",
    "        # Quasi Newton to improve the result : model == updated model\n",
    "        print(f'\\n Initial loss : {loss(t)}')\n",
    "        shapes = tf.shape_n(model.trainable_variables)\n",
    "        length_shapes = len(shapes)\n",
    "        count = 0\n",
    "        idx = []\n",
    "        part = []\n",
    "        for i in range(length_shapes):\n",
    "            n = np.product(shapes[i])\n",
    "            idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "            count += n   \n",
    "            part.extend([i]*n)\n",
    "\n",
    "        def update_weights(OneD):\n",
    "            params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        def flatten_the_parameters(weights):\n",
    "            return tf.dynamic_stitch(idx, weights) \n",
    "\n",
    "        @tf.function\n",
    "        def value_and_gradient(flat_weights):\n",
    "            #update weights \n",
    "            update_weights(flat_weights)\n",
    "            with tf.GradientTape() as g:\n",
    "                los = loss(t)\n",
    "\n",
    "            grad = g.gradient(los, model.trainable_variables) \n",
    "            flattend_grad = flatten_the_parameters(grad) #converts from multDim to 1D\n",
    "            return los, flattend_grad\n",
    "\n",
    "        start_time2 = time.time()\n",
    "\n",
    "        flat_parameters = flatten_the_parameters(model.trainable_variables)\n",
    "        start = tf.constant(flat_parameters)  # Starting point for the search.\n",
    "        grads_lists = []\n",
    "        optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "          value_and_gradient, initial_position=start, tolerance=1.0e-20, max_iterations=5000)\n",
    "        grad_curr = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "        grads_lists.append(grad_curr)\n",
    "\n",
    "        while  optim_results.converged.numpy() == False:\n",
    "            optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "          value_and_gradient, initial_position=start, max_iterations=5000)\n",
    "            print('converged:', optim_results.converged.numpy())\n",
    "            print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "            print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "            print('')\n",
    "            new_grad = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "\n",
    "            if new_grad == grads_lists[-1]:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        print('converged:', optim_results.converged.numpy())\n",
    "        print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "        print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "        update_weights(optim_results.position)\n",
    "\n",
    "    #         print('initial loss',initial_loss_after)\n",
    "        print('loss',loss(t))\n",
    "        print('Times : ',time.time()-start_time2,'\\n')      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         return total_loss, total_loss_updated\n",
    "#             print(f'Layer no : {n} \\n Corresponding derivative :\\n {grad_residual_x[n]} ')\n",
    "#             print('====================================================================== \\n')\n",
    "                \n",
    "                \n",
    "        '''       Before the website : https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs/blob/master/Helmholtz/Helmholtz2D_model_tf.py\n",
    "\n",
    "                print(f'shape residue with 0 {grad_residual_x[n][0].shape} \\n Without zero {grad_residual_x[n].shape}')\n",
    "                lambda_hat_real = self.lambda_hat(grad_residual_x[n][0],grad_mse_x0_real[n][0])\n",
    "                lambda_hat_imag = self.lambda_hat(grad_residual_y[n][0],grad_mse_y0_imag[n][0])\n",
    "\n",
    "                print(f'lambda_hat_real :{lambda_hat_real} \\n lambda_hat_imag {lambda_hat_imag}')\n",
    "\n",
    "\n",
    "                lambda_real = (1. - alpha)*lambda_real + alpha*lambda_hat_real\n",
    "                lambda_imag = (1. - alpha)*lambda_imag + alpha*lambda_hat_imag \n",
    "\n",
    "                update_real = eta*(grad_residual_x[n][0]+lambda_real*grad_mse_x0_real[n][0])\n",
    "                update_imag = eta*(grad_residual_y[n][0]+lambda_imag*grad_mse_y0_imag[n][0]) \n",
    "                total_update = update_real + update_imag\n",
    "                print(f'update :{total_update.shape,self.model.trainable_variables[n].shape }')\n",
    "\n",
    "                total_update = tf.reshape(total_update,self.model.trainable_variables[n].shape)\n",
    "#                 print(f'update :{total_update.shape,self.model.trainable_variables[n].shape }')\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            else: \n",
    "                plt.plot(grad_residual_x[n],label = 'grad_res')\n",
    "                plt.plot(grad_mse_x0_real[n],label = 'grad_IC')\n",
    "                plt.title('layer = '+str(n))\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f'shape residue with 0 {grad_residual_x[n][0].shape} \\n Without zero {grad_residual_x[n].shape}')\n",
    "            \n",
    "            \n",
    "                lambda_hat_real = self.lambda_hat(grad_residual_x[n],grad_mse_x0_real[n])\n",
    "                lambda_hat_imag = self.lambda_hat(grad_residual_y[n],grad_mse_y0_imag[n])\n",
    "\n",
    "                print(f'lambda_hat_real :{lambda_hat_real} \\n lambda_hat_imag {lambda_hat_imag}')\n",
    "\n",
    "\n",
    "                lambda_real = (1. - alpha)*lambda_real + alpha*lambda_hat_real\n",
    "                lambda_imag = (1. - alpha)*lambda_imag + alpha*lambda_hat_imag \n",
    "\n",
    "                update_real = eta*(grad_residual_x[n]+lambda_real*grad_mse_x0_real[n])\n",
    "                update_imag = eta*(grad_residual_y[n]+lambda_imag*grad_mse_y0_imag[n]) \n",
    "                total_update = update_real + update_imag\n",
    "            \n",
    "            self.model.trainable_variables[n].assign_sub(total_update)\n",
    "            \n",
    "            \n",
    "            if tf.size(grad_residual_x[n].shape) == 2:\n",
    "                plt.plot(grad_residual_x[n][0],label = 'After_grad_res')\n",
    "                plt.plot(lambda_real*grad_mse_x0_real[n][0],label = 'Aftergrad_IC')\n",
    "                plt.title('layer_corrected = '+str(n))\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "            else:        \n",
    "                plt.plot(grad_residual_x[n],label = 'After_grad_res')\n",
    "                plt.plot(lambda_real*grad_mse_x0_real[n],label = 'After_grad_IC')\n",
    "                plt.title('layer = '+str(n))\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "        residual_x,residual_y,mse_x0_real,mse_y0_imag = self.loss(t)\n",
    "        new_loss = residual_x + mse_x0_real*lambda_real +   residual_y + mse_y0_imag*lambda_imag\n",
    "        '''        \n",
    "\n",
    "#             print('updated_loss ',new_loss )\n",
    "#         return new_loss    \n",
    "            \n",
    "#     def train_with_annealing_update(self,iterations, t):\n",
    "#         loss_f = []\n",
    "#         for i in tnrange(iterations):\n",
    "#             loss_f.append(self.annealing_update(t))\n",
    "#         return loss_f   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.linspace(0,1,10**5)[:,tf.newaxis]\n",
    "t.shape\n",
    "optim = tf.keras.optimizers.Adam()\n",
    "z_hat = Z_approx(num_of_hid_layers = 3, units_per_layer = 30, num_out=2, initialization = 0,activation = 'mish')\n",
    "z_hat.build(t.shape)\n",
    "z_hat(t)\n",
    "x_int = tf.constant(1.0, dtype = tf.float32)\n",
    "y_int = tf.constant(0.0, dtype = tf.float32)\n",
    "\n",
    "z_init = tf.constant(np.array([x_int,y_int]), dtype=tf.float32)\n",
    "pinn = PINN(t,z_hat,z_init, optim)\n",
    "\n",
    "pinn.annealing_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd79fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,best_loss = pinn.train_whole_set(t,200,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1943edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.plot(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a53cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59052ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= z_hat\n",
    "model.trainable_variables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.train_and_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84981541",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = tf.shape_n(model.trainable_variables)\n",
    "length_shapes = len(shapes)\n",
    "count = 0\n",
    "idx = []\n",
    "part = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(length_shapes):\n",
    "    n = np.product(shapes[i])\n",
    "    idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "    count += n   \n",
    "    part.extend([i]*n)\n",
    "    \n",
    "def update_weights(OneD):\n",
    "    params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "    for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "        model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "def flatten_the_parameters(weights):\n",
    "    return tf.dynamic_stitch(idx, weights) \n",
    "\n",
    "@tf.function\n",
    "def value_and_gradient(flat_weights):\n",
    "    #update weights \n",
    "    update_weights(flat_weights)\n",
    "    with tf.GradientTape() as g:\n",
    "        los = pinn.train_and_update(t)\n",
    "\n",
    "    grad = g.gradient(los, model.trainable_variables) \n",
    "    flattend_grad = flatten_the_parameters(grad) #converts from multDim to 1D\n",
    "    return los, flattend_grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82762970",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_and_gradient(flatten_the_parameters(model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_parameters = flatten_the_parameters(model.trainable_variables)\n",
    "start = tf.constant(flat_parameters)  # Starting point for the search.\n",
    "grads_lists = []\n",
    "optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "  value_and_gradient, initial_position=start, tolerance=1.0e-20, max_iterations=5000)\n",
    "# grad_curr = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "# grads_lists.append(grad_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_results.converged.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333054b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "while  optim_results.converged.numpy() == False:\n",
    "    optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "  value_and_gradient, initial_position=start)\n",
    "    print('converged:', optim_results.converged.numpy())\n",
    "    print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "    print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "    print('')\n",
    "    new_grad = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "\n",
    "    if new_grad == grads_lists[-1]:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train_lbfgs(self,model,loss,t):\n",
    "        # Quasi Newton to improve the result : model == updated model\n",
    "        print(f'\\n Initial loss : {loss(t)}')\n",
    "        shapes = tf.shape_n(model.trainable_variables)\n",
    "        length_shapes = len(shapes)\n",
    "        count = 0\n",
    "        idx = []\n",
    "        part = []\n",
    "        for i in range(length_shapes):\n",
    "            n = np.product(shapes[i])\n",
    "            idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "            count += n   \n",
    "            part.extend([i]*n)\n",
    "\n",
    "        def update_weights(OneD):\n",
    "            params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        def flatten_the_parameters(weights):\n",
    "            return tf.dynamic_stitch(idx, weights) \n",
    "\n",
    "        @tf.function\n",
    "        def value_and_gradient(flat_weights):\n",
    "            #update weights \n",
    "            update_weights(flat_weights)\n",
    "            with tf.GradientTape() as g:\n",
    "                los = loss(t)\n",
    "\n",
    "            grad = g.gradient(los, model.trainable_variables) \n",
    "            flattend_grad = flatten_the_parameters(grad) #converts from multDim to 1D\n",
    "            return los, flattend_grad\n",
    "\n",
    "        start_time2 = time.time()\n",
    "\n",
    "        flat_parameters = flatten_the_parameters(model.trainable_variables)\n",
    "        start = tf.constant(flat_parameters)  # Starting point for the search.\n",
    "        grads_lists = []\n",
    "        optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "          value_and_gradient, initial_position=start, tolerance=1.0e-20, max_iterations=5000)\n",
    "        grad_curr = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "        grads_lists.append(grad_curr)\n",
    "\n",
    "        while  optim_results.converged.numpy() == False:\n",
    "            optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "          value_and_gradient, initial_position=start, max_iterations=5000)\n",
    "            print('converged:', optim_results.converged.numpy())\n",
    "            print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "            print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "            print('')\n",
    "            new_grad = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "\n",
    "            if new_grad == grads_lists[-1]:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        print('converged:', optim_results.converged.numpy())\n",
    "        print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "        print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "        update_weights(optim_results.position)\n",
    "\n",
    "    #         print('initial loss',initial_loss_after)\n",
    "        print('loss',loss(t))\n",
    "        print('Times : ',time.time()-start_time2,'\\n')      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_sum(pinn.loss(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ab5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_and_update(t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        los  = pinn.annealing_update(t)\n",
    "    dlossdpa = tape.gradient(los,z_hat.trainable_variables) \n",
    "    optim.apply_gradients(zip(dlossdpa, z_hat.trainable_variables))\n",
    "    return los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b52696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'initial loss :{pinn.annealing_update(t)}')\n",
    "for i in tnrange(20):\n",
    "    new_loss = train_and_update(t)\n",
    "    print(f'New loss {new_loss.numpy()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23218a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    def __init__(self, t, model,z_init,optim):\n",
    "        self.model = model\n",
    "        self.t = t\n",
    "        self.x_init = z_init[0]\n",
    "        self.y_init = z_init[1]\n",
    "        self.optim = optim\n",
    "        \n",
    "        n_ic = tf.constant(2, tf.int32)\n",
    "        lambdas = tf.Variable(tf.ones(n_ic),)\n",
    "        lambdas_hat = tf.Variable(tf.zeros(n_ic))\n",
    "\n",
    "        \n",
    "    def get_derivatives(self,t):\n",
    "        '''\n",
    "        Calculate the derivatives and function that correspond to the ode\n",
    "\n",
    "        input:\n",
    "                t : independent variant\n",
    "\n",
    "        output:\n",
    "                value and corresponding derivative\n",
    "\n",
    "        '''\n",
    "        \n",
    "#         z = u + iv\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(t)         \n",
    "            u = self.model(t)[:,0:1]\n",
    "            v = self.model(t)[:,1:2]\n",
    "        dudt = tf.cast(tape.gradient(u,t),dtype=tf.float32)\n",
    "        dvdt = tf.cast(tape.gradient(v,t),dtype=tf.float32)  \n",
    "        del tape\n",
    "        \n",
    "        return u,v, dudt,dvdt\n",
    "    \n",
    "    def equations(self, u,v,dudt,dvdt):\n",
    "    #     derivatives must be dtype tf.float32\n",
    "    #     dzdt = iz(t)\n",
    "        dzdt = tf.complex(dudt,dvdt) # dzdt = dudt + idvdt . \n",
    "        z = tf.complex(u,v)\n",
    "        rhs = z*1.j\n",
    "\n",
    "    #     real(LHS)-Real(RHS) = -Imag(LHS)+Img(RHS) Equation from PINN paper\n",
    "\n",
    "        f_real = tf.math.real(dzdt) - tf.math.real(rhs) # = real(LHS)-Real(RHS)\n",
    "        f_imag = -tf.math.imag(dzdt) + tf.math.imag(rhs) # = -Imag(LHS)+Img(RHS)\n",
    "        return f_real,f_imag\n",
    "        \n",
    "    def ode_ic(self,t):\n",
    "        u,v, dudt,dvdt  = self.get_derivatives(t)\n",
    "        f_real,f_imag = self.equations(u,v, dudt,dvdt)\n",
    "        u1 =  f_real   #residual of real numbers\n",
    "        u2 =  f_imag   #residual of Imag numbers\n",
    "#         Initial conditions\n",
    "        u0,v0,_,_ = self.get_derivatives(tf.zeros_like(t))\n",
    "#        \n",
    "        u3 = u0 - self.x_init\n",
    "        u4 = v0  - self.y_init\n",
    "\n",
    "        return u1,u2,u3,u4\n",
    "        \n",
    "    def loss(self,t):\n",
    "        u1,u2,u3,u4 = self.ode_ic(t)\n",
    "        mse_x  = tf.reduce_mean(tf.square(u1))\n",
    "        mse_y  = tf.reduce_mean(tf.square(u2))\n",
    "        mse_x0 = tf.reduce_mean(tf.square(u3))\n",
    "        mse_y0 = tf.reduce_mean(tf.square(u4))\n",
    "        print(f'mse_x :{mse_x}, mse_y : {mse_y}')\n",
    "        print('Same order as bc')\n",
    "        print(f'mse_x0 :{mse_x0}, mse_y0 : {mse_y0}')\n",
    "        return mse_x+mse_y+mse_x0+mse_y0\n",
    "    \n",
    "    def make_flat(self,gradients):\n",
    "        shapes = tf.shape_n(gradients)\n",
    "        length_shapes = len(shapes)\n",
    "        count = 0\n",
    "        idx = []\n",
    "        part = []\n",
    "        for i in range(length_shapes):\n",
    "            n = np.product(shapes[i])\n",
    "            idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "            count += n   \n",
    "            part.extend([i]*n)\n",
    "\n",
    "        flattened =  tf.math.abs(tf.dynamic_stitch(idx, gradients))\n",
    "        return flattened\n",
    "    \n",
    "    def calc_lambda_hat(self, grad_residue_x,grad_residue_x_ic):\n",
    "        eps = tf.constant(1e-15, tf.float32)\n",
    "        numerator     = tf.reduce_max(tf.math.abs(grad_residue_x))\n",
    "        denominator   = tf.math.abs(grad_residue_x_ic)\n",
    "        denominator   = tf.math.reduce_mean(denominator)\n",
    "        return tf.cast(numerator/(denominator+eps),tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_and_update(self,t):\n",
    "        lsso = []\n",
    "#         n_ic = tf.constant(2, tf.int32)\n",
    "#         lambdas = tf.Variable(tf.ones(n_ic))\n",
    "#         lambdas_hat = tf.Variable(tf.zeros(n_ic))\n",
    "        alpha = tf.constant(0.9, tf.float32)\n",
    "        eta = tf.constant(10**(-3), tf.float32)\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            u1,u2,u3,u4 = self.ode_ic(t)\n",
    "            mse_x  = tf.reduce_mean(tf.square(u1))\n",
    "            mse_y  = tf.reduce_mean(tf.square(u2))\n",
    "            mse_x0 = tf.reduce_mean(tf.square(u3))\n",
    "            mse_y0 = tf.reduce_mean(tf.square(u4))\n",
    "            \n",
    "        grad_residue_x = tape.gradient(mse_x, self.model.trainable_variables)\n",
    "        grad_residue_x_ic = tape.gradient(mse_x0, self.model.trainable_variables)\n",
    "        \n",
    "        grad_residue_y = tape.gradient(mse_y, self.model.trainable_variables)\n",
    "        grad_residue_y_ic = tape.gradient(mse_y0, self.model.trainable_variables)     \n",
    "        del tape \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(f' Initalloss :{self.loss(t)}')\n",
    "        total_n = len(tf.shape_n(self.model.trainable_variables))\n",
    "        for n in range(total_n):\n",
    "            \n",
    "            lambdas_hat[0].assign(self.calc_lambda_hat(grad_residue_x[n],grad_residue_x_ic[n]))\n",
    "            lambdas[0].assign((1. - alpha)*lambdas[0] + alpha*lambdas_hat[0])\n",
    "            \n",
    "            lambdas_hat[1].assign(self.calc_lambda_hat(grad_residue_y[n],grad_residue_y_ic[n]))\n",
    "            lambdas[1].assign((1. - alpha)*lambdas[1] + alpha*lambdas_hat[1])\n",
    "            \n",
    "            loss_x = mse_x + lambdas[0]*mse_x0\n",
    "            loss_y = mse_y + lambdas[1]*mse_y0\n",
    "            \n",
    "            loss_total = loss_x + loss_y \n",
    "            weight_b4 = self.model.trainable_variables[n]\n",
    "            update = eta*(grad_residue_x[n] + lambdas[0]*grad_residue_x_ic[n] + grad_residue_y[n]+lambdas[1]*grad_residue_y_ic[n])\n",
    "            self.model.trainable_variables[n].assign_sub(update)\n",
    "            lsso.append(loss_total)\n",
    "            weight_after = self.model.trainable_variables[n]\n",
    "        return weight_b4,weight_after\n",
    "            \n",
    "#\n",
    "        \n",
    "        \n",
    "#         return mse_x,mse_x0,grad_residue_x,grad_residue_x_ic,mse_y,mse_y0,grad_residue_y,grad_residue_y_ic\n",
    "            \n",
    "    \n",
    "#     @tf.function\n",
    "#     def loss_and_update(self,t):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             los  = self.loss(t)\n",
    "#         dlossdpa = tape.gradient(los,self.model.trainable_variables) \n",
    "#         self.optim.apply_gradients(zip(dlossdpa, self.model.trainable_variables))\n",
    "#         return los\n",
    "    \n",
    "    def train_with_adam(self,batch_mode,epochs,batch_size,t):\n",
    "        @tf.function\n",
    "        def train_and_update(t):\n",
    "    #         print(f'params : \\n {self.model.trainable_variables}')\n",
    "            with tf.GradientTape() as tape:\n",
    "#                 tape.watch(model.trainable_variables)\n",
    "#                 los  = self.loss(t)   Original\n",
    "                los  = self.loss_and_update(t)   #Modified\n",
    "\n",
    "            dlossdpa = tape.gradient(los,self.model.trainable_variables) \n",
    "            self.optim.apply_gradients(zip(dlossdpa, self.model.trainable_variables))\n",
    "#             print(f'\\n Loss after update :{self.loss(t).numpy()}\\n')\n",
    "            return los\n",
    "        \n",
    "        def train_and_update_batch(epochs, batch_size,t):\n",
    "            loss_batch = []\n",
    "            loss_decrease = []\n",
    "            steps_per_epoch = int(t.shape[0]/batch_size)\n",
    "            \n",
    "            initial_loss = train_and_update(t).numpy()\n",
    "            best_so_far = np.inf\n",
    "#             print(f'initial loss : {initial_loss}\\n')\n",
    "            loss_batch.append(initial_loss)\n",
    "            \n",
    "            for epoch in tnrange(epochs, desc= 'EPOCHS'):\n",
    "                for i in range(steps_per_epoch):\n",
    "                    t_batch = t[i*batch_size:batch_size*(i+1)]\n",
    "                    loss = train_and_update(t_batch).numpy()\n",
    "                    \n",
    "                    \n",
    "                    if loss < loss_batch[-1]:\n",
    "                        if loss < best_so_far:\n",
    "                            best_so_far = loss\n",
    "                            loss_decrease.append(loss)\n",
    "                            self.model.save_weights('best_model_batch')\n",
    "                            \n",
    "                    loss_batch.append(loss)  \n",
    "                    \n",
    "                if epoch%1000==0:   \n",
    "                    print(f'\\n loss for epoch {epoch} : {loss}  best so far {best_so_far}')  \n",
    "            print(f'Final loss : {loss} best so far {best_so_far}')    \n",
    "            return loss_batch,loss_decrease      \n",
    "                        \n",
    "                        \n",
    "        def train_whole_set(t):                \n",
    "            losses =[]\n",
    "            loss_decrease = []\n",
    "            check_decrease = 0\n",
    "            initial_loss = train_and_update(t).numpy()\n",
    "            best_so_far =  np.inf\n",
    "            print(f'initial loss : {initial_loss}')\n",
    "            losses.append(initial_loss)\n",
    "\n",
    "            for epoch in tnrange(epochs):\n",
    "                loss = train_and_update(t).numpy()\n",
    "                \n",
    "                if loss < losses[-1]:\n",
    "                    if loss < best_so_far:\n",
    "                        best_so_far = loss\n",
    "                        loss_decrease.append(loss)\n",
    "                        self.model.save_weights('best_model')\n",
    "                        check_decrease = loss\n",
    "                        \n",
    "                losses.append(loss)\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                if epoch%1000==0:\n",
    "                    print(f'loss for epoch {epoch} : {loss} best so far {best_so_far}') \n",
    "            #         print(end = \".\")\n",
    "            \n",
    "            self.model.load_weights('best_model')\n",
    "            train_lbfgs(self.model,train_and_update)\n",
    "            \n",
    "#             print(f'Final loss : {loss}  best so far {best_so_far}') \n",
    "#             print(f'Last decrease : {check_decrease}')\n",
    "            return losses,loss_decrease\n",
    "        \n",
    "        if batch_mode == True:\n",
    "            losses,loss_decrease = train_and_update_batch(epochs, batch_size,t)\n",
    "        else:\n",
    "            losses,loss_decrease = train_whole_set(t)\n",
    "            \n",
    "        return losses , loss_decrease  \n",
    "    \n",
    "def train_lbfgs(model,loss):\n",
    "    # Quasi Newton to improve the result : model == updated model\n",
    "    print(f'\\n Initial loss : {loss(t)}')\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    length_shapes = len(shapes)\n",
    "    count = 0\n",
    "    idx = []\n",
    "    part = []\n",
    "    for i in range(length_shapes):\n",
    "        n = np.product(shapes[i])\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "        count += n   \n",
    "        part.extend([i]*n)\n",
    "\n",
    "    def update_weights(OneD):\n",
    "        params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    def flatten_the_parameters(weights):\n",
    "        return tf.dynamic_stitch(idx, weights) \n",
    "\n",
    "    @tf.function\n",
    "    def value_and_gradient(flat_weights):\n",
    "        #update weights \n",
    "        update_weights(flat_weights)\n",
    "        with tf.GradientTape() as g:\n",
    "            los = loss(t)\n",
    "\n",
    "        grad = g.gradient(los, model.trainable_variables) \n",
    "        flattend_grad = flatten_the_parameters(grad) #converts from multDim to 1D\n",
    "        return los, flattend_grad\n",
    "\n",
    "    start_time2 = time.time()\n",
    "\n",
    "    flat_parameters = flatten_the_parameters(model.trainable_variables)\n",
    "    start = tf.constant(flat_parameters)  # Starting point for the search.\n",
    "    grads_lists = []\n",
    "    optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "      value_and_gradient, initial_position=start, tolerance=1.0e-20, max_iterations=5000)\n",
    "    grad_curr = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "    grads_lists.append(grad_curr)\n",
    "\n",
    "    while  optim_results.converged.numpy() == False:\n",
    "        optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "      value_and_gradient, initial_position=start, max_iterations=5000)\n",
    "        print('converged:', optim_results.converged.numpy())\n",
    "        print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "        print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "        print('')\n",
    "        new_grad = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "\n",
    "        if new_grad == grads_lists[-1]:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    print('converged:', optim_results.converged.numpy())\n",
    "    print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "    print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "    update_weights(optim_results.position)\n",
    "\n",
    "#         print('initial loss',initial_loss_after)\n",
    "    print('loss',loss(t))\n",
    "    print('Times : ',time.time()-start_time2,'\\n')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524778a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam()\n",
    "# epochs = 5000\n",
    "epochs = 5000\n",
    "batch = 100\n",
    "x_int = tf.constant(1.0, dtype = tf.float32)\n",
    "y_int = tf.constant(0.0, dtype = tf.float32)\n",
    "\n",
    "z_init = tf.constant(np.array([x_int,y_int]), dtype=tf.float32)\n",
    "# New idea z = [x,y]\n",
    "pinn = PINN(t, z_hat,z_init,optim)\n",
    "\n",
    "pinn.loss_and_update(t)\n",
    "\n",
    "# start = time.time()\n",
    "# losss,loss_decrease =  pinn.train_with_adam(batch_mode=False,epochs=epochs,batch_size=batch,t=t)\n",
    "# print(f'Total time {time.time() -start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True solutions\n",
    "x_true = lambda t : tf.math.cos(t)\n",
    "y_true = lambda t : tf.math.sin(t)\n",
    "\n",
    "plt.plot(t, x_true(t) ,label = 'real')\n",
    "plt.plot(t,  y_true(t), label = 'imag')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, x_true(t) ,label = 'Real_soln')\n",
    "plt.plot(t, z_hat(t)[:,0:1], label = 'NN_soln')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(t, y_true(t) ,label = 'Imag_soln')\n",
    "plt.plot(t, z_hat(t)[:,1:], label = 'NN_soln')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11e8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de3406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2d533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a729d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdce58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5de370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c689b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fca391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4b679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09f376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af60e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba1f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a53532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abdaf69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812160a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14459430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154ad38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f4bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda6eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed73ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea320ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e9097b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0941688b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93aa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d00dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfac3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b25126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cadfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e82151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c2b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3109adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.loss_and_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58260232",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam()\n",
    "batch = 100\n",
    "# optim = tf.keras.optimizers.Adam(lr)\n",
    "pinn = PINN(t, z_hat,z_init,optim)\n",
    "start = time.time()\n",
    "losss,l_decrease =  pinn.train_with_adam(batch_mode=True,epochs=epochs,batch_size=batch,t=t)\n",
    "print(f'final loss {losss[-1]} in is {(time.time() -start)//60.}s ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0211270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553b48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1147fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ac662",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam()\n",
    "x_int = tf.constant(1.0, dtype = tf.float32)\n",
    "y_int = tf.constant(0.0, dtype = tf.float32)\n",
    "z_init = tf.constant(np.array([x_int,y_int]), dtype=tf.float32)\n",
    "# New idea z = [x,y]\n",
    "pinn = PINN(t, z_hat,z_init,optim)\n",
    "# mse_x,mse_x0,grad_residue_x,grad_residue_x_ic,mse_y,mse_y0,grad_residue_y,grad_residue_y_ic = pinn.loss_and_train(t)\n",
    "# print(len(tf.shape_n(grad_residue_x)),len(tf.shape_n(grad_residue_x_ic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2363b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.loss_and_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ba92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pinn.loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses =[]\n",
    "loss_decrease = []\n",
    "check_decrease = 0\n",
    "initial_loss = pinn.loss_and_update(t).numpy()\n",
    "best_so_far =  np.inf\n",
    "print(f'initial loss : {initial_loss}')\n",
    "losses.append(initial_loss)\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in tnrange(epochs):\n",
    "    loss = pinn.loss_and_update(t).numpy()\n",
    "\n",
    "    if loss < losses[-1]:\n",
    "        if loss < best_so_far:\n",
    "            print('loss',loss)\n",
    "            best_so_far = loss\n",
    "            loss_decrease.append(loss)\n",
    "            self.model.save_weights('best_model')\n",
    "            check_decrease = loss\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "\n",
    "\n",
    "    if epoch%1000==0:\n",
    "        print(f'loss for epoch {epoch} : {loss} best so far {best_so_far}') \n",
    "#         print(end = \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e43b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bone[0].assign(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lambda_hat(grad_residue_x,grad_residue_x_ic):\n",
    "    numerator     = tf.reduce_max(tf.math.abs(grad_residue_x))\n",
    "    denominator   = tf.math.abs(grad_residue_x_ic)\n",
    "    denominator   = tf.math.reduce_mean(denominator)\n",
    "    return numerator/(denominator_1+1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243c8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ic = 2\n",
    "lambdas = np.ones(n_ic)\n",
    "lambdas_hat = np.zeros(n_ic)\n",
    "total_n = len(tf.shape_n(grad_residue_x))\n",
    "alpha = 0.9\n",
    "eta = 10**(-3)\n",
    "print(f' Initalloss :{pinn.loss(t)}')\n",
    "for n in range(total_n):\n",
    "    for i in range(n_ic):\n",
    "        lambdas_hat[i] = calc_lambda_hat(grad_residue_x[n],grad_residue_x_ic[n])\n",
    "        lambdas[i] = (1. - alpha)*lambdas[i] + alpha*lambdas_hat[i]\n",
    "#     print(n,lambdas)  \n",
    "    \n",
    "#     print(mse_x.numpy(),lambdas[0]*mse_x0.numpy())\n",
    "#     print(mse_y.numpy(),lambdas[1]*mse_y0.numpy())\n",
    "    theta_update = eta*(grad_residue_x[n]+grad_residue_y[n] +lambdas[0]*mse_x0 + lambdas[1]*mse_y0)\n",
    "    \n",
    "#     print(theta_update)\n",
    "    pinn.model.trainable_variables[n].assign_sub(theta_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cfc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c219c779",
   "metadata": {},
   "outputs": [],
   "source": [
    " def train_lbfgs(model,loss):\n",
    "        # Quasi Newton to improve the result : model == updated model\n",
    "        print(f'\\n Initial loss : {loss(t)}')\n",
    "        shapes = tf.shape_n(model.trainable_variables)\n",
    "        length_shapes = len(shapes)\n",
    "        count = 0\n",
    "        idx = []\n",
    "        part = []\n",
    "        for i in range(length_shapes):\n",
    "            n = np.product(shapes[i])\n",
    "            idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "            count += n   \n",
    "            part.extend([i]*n)\n",
    "\n",
    "        def update_weights(OneD):\n",
    "            params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "                model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "        def flatten_the_parameters(weights):\n",
    "            return tf.dynamic_stitch(idx, weights) \n",
    "\n",
    "        @tf.function\n",
    "        def value_and_gradient(flat_weights):\n",
    "            #update weights \n",
    "            update_weights(flat_weights)\n",
    "            with tf.GradientTape() as g:\n",
    "                los = loss(t)\n",
    "\n",
    "            grad = g.gradient(los, model.trainable_variables) \n",
    "            flattend_grad = flatten_the_parameters(grad) #converts from multDim to 1D\n",
    "            return los, flattend_grad\n",
    "\n",
    "        start_time2 = time.time()\n",
    "\n",
    "        flat_parameters = flatten_the_parameters(model.trainable_variables)\n",
    "        start = tf.constant(flat_parameters)  # Starting point for the search.\n",
    "        grads_lists = []\n",
    "        optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "          value_and_gradient, initial_position=start, tolerance=1.0e-20, max_iterations=5000)\n",
    "        grad_curr = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "        grads_lists.append(grad_curr)\n",
    "\n",
    "        while  optim_results.converged.numpy() == False:\n",
    "            optim_results = tfp.optimizer.lbfgs_minimize(\n",
    "          value_and_gradient, initial_position=start, max_iterations=5000)\n",
    "            print('converged:', optim_results.converged.numpy())\n",
    "            print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "            print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "            print('')\n",
    "            new_grad = tf.norm(optim_results.objective_gradient,ord=np.inf)\n",
    "           \n",
    "            if new_grad == grads_lists[-1]:\n",
    "                break\n",
    "                \n",
    "\n",
    "            \n",
    "        print('converged:', optim_results.converged.numpy())\n",
    "        print(f'num_iterations {optim_results.num_iterations} and evaluations {optim_results.num_objective_evaluations}')\n",
    "        print(f'gradient:{tf.norm(optim_results.objective_gradient,ord=np.inf)}')\n",
    "        update_weights(optim_results.position)\n",
    "        \n",
    "#         print('initial loss',initial_loss_after)\n",
    "        print('loss',loss(t))\n",
    "        print('Times : ',time.time()-start_time2,'\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60230a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff099a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2773bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_flat(tt):\n",
    "    shapes = tf.shape_n(tt)\n",
    "    length_shapes = len(shapes)\n",
    "    count = 0\n",
    "    idx = []\n",
    "    part = []\n",
    "    for i in range(length_shapes):\n",
    "        n = np.product(shapes[i])\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "        count += n   \n",
    "        part.extend([i]*n)\n",
    "        \n",
    "    return tf.dynamic_stitch(idx, tt)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a03bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_flat(pinn.loss_and_train(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003bbd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pinn.loss_and_train(t)\n",
    "shapes = tf.shape_n(tt)\n",
    "length_shapes = len(shapes)\n",
    "count = 0\n",
    "idx = []\n",
    "part = []\n",
    "for i in range(length_shapes):\n",
    "    n = np.product(shapes[i])\n",
    "    idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "    count += n   \n",
    "    part.extend([i]*n)\n",
    "    \n",
    "def flatten_the_parameters(weights):\n",
    "    return tf.dynamic_stitch(idx, weights)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a39892",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(flatten_the_parameters(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc74a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam()\n",
    "# epochs = 5000\n",
    "epochs = 1000\n",
    "batch = 100\n",
    "x_int = tf.constant(1.0, dtype = tf.float32)\n",
    "y_int = tf.constant(0.0, dtype = tf.float32)\n",
    "\n",
    "z_init = tf.constant(np.array([x_int,y_int]), dtype=tf.float32)\n",
    "# New idea z = [x,y]\n",
    "pinn = PINN(t, z_hat,z_init,optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c60d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c86b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ce88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_mode=False means without batches\n",
    "start = time.time()\n",
    "losss,loss_decrease =  pinn.train_with_adam(batch_mode=False,epochs=epochs,batch_size=batch,t=t)\n",
    "print(f'final loss {losss[-1]} in {time.time() -start} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcf98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.loss_and_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.loss_and_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hat.load_weights('best_model')\n",
    "pinn_best.loss_and_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44871d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hat.load_weights('best_model')\n",
    "pinn_best = PINN(t, z_hat,z_init,optim)\n",
    "pinn_best.loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a90fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_best.loss_and_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75181693",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hat.trainable_variables[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd761b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_hat = PINN(t, z_hat,z_init,optim).loss_and_update(t)\n",
    "l_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = PINN(t, model,z_init,optim)\n",
    "pinn.loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lbfgs(model,pinn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02594cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.loss(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Quasi Newton to improve the result : model == updated model\n",
    "model =  z_hat  \n",
    "\n",
    "shapes = tf.shape_n(model.trainable_variables)\n",
    "length_shapes = len(shapes)\n",
    "count = 0\n",
    "idx = []\n",
    "part = []\n",
    "for i in range(length_shapes):\n",
    "    n = np.product(shapes[i])\n",
    "    idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "    count += n   \n",
    "    part.extend([i]*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(OneD):\n",
    "    params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "    for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "        model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "        \n",
    "def flatten_the_parameters(weights):\n",
    "    return tf.dynamic_stitch(idx, weights)         \n",
    "\n",
    "@tf.function\n",
    "def value_and_gradient(flat_weights):\n",
    "    #update weights \n",
    "    update_weights(flat_weights)\n",
    "    with tf.GradientTape() as g:\n",
    "        los = PINN(t, model,z_init,optim).loss(t)\n",
    "    grad = g.gradient(los, model.trainable_variables) \n",
    "    flattend_grad = flatten_the_parameters(grad) #converts from multDim to 1D\n",
    "    return los, flattend_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df67968",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_results.objective_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797441cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = tf.shape_n(model.trainable_variables)\n",
    "length_shapes = len(shapes)\n",
    "count = 0\n",
    "idx = []\n",
    "part = []\n",
    "for i in range(length_shapes):\n",
    "    n = np.product(shapes[i])\n",
    "    idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32),shapes[i]))\n",
    "    count += n   \n",
    "    part.extend([i]*n)\n",
    "    \n",
    "def flatten_the_parameters(weights):\n",
    "    return tf.dynamic_stitch(idx, weights) \n",
    "oneD = flatten_the_parameters(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(OneD):\n",
    "    params = tf.dynamic_partition(OneD, part, length_shapes)\n",
    "    for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "        model.trainable_variables[i].assign(tf.reshape(param, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a395a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_weights(oneD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd61d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True solutions\n",
    "x_true = lambda t : tf.math.cos(t)\n",
    "y_true = lambda t : tf.math.sin(t)\n",
    "\n",
    "plt.plot(t, x_true(t) ,label = 'real')\n",
    "plt.plot(t,  y_true(t), label = 'imag')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515469f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, x_true(t) ,label = 'Real_soln')\n",
    "plt.plot(t, z_hat(t)[:,0:1], label = 'NN_soln')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45969571",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, y_true(t) ,label = 'Imag_soln')\n",
    "plt.plot(t, z_hat(t)[:,1:], label = 'NN_soln')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With batches\n",
    "# lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
    "optim = tf.keras.optimizers.Adam()\n",
    "batch = 10\n",
    "# optim = tf.keras.optimizers.Adam(lr)\n",
    "pinn = PINN(t, z_hat,z_init,optim)\n",
    "start = time.time()\n",
    "losss,l_decrease =  pinn.train_with_adam(batch_mode=True,epochs=epochs,batch_size=batch,t=t)\n",
    "print(f'final loss {losss[-1]} in is {(time.time() -start)//60.}s ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PINN(t, z_hat.load_weights('best_model_batch'),z_init,optim).loss_and_update(t)\n",
    "z_hat.weights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b32d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hat.load_weights('best_model_batch')\n",
    "z_hat.weights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14cf1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hat.weights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN(t, z_hat,z_init,optim).loss_and_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b768905",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_hat.load_weights('best_model')\n",
    "pinn = PINN(t, z_hat,z_init,optim)\n",
    "pinn.loss_and_update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, y_true(t) ,label = 'Imag_soln')\n",
    "plt.plot(t, z_hat(t)[:,1:], label = 'NN_soln')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(z_hat(t)[:,0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, x_true(t) ,label = 'Real')\n",
    "plt.plot(t, z_hat(t)[:,0:1], label = 'NN')\n",
    "# plt.ylim(np.min(z_hat(t)[:,0:1]),)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d04b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(t, y_true(t) ,label = 'Real')\n",
    "plt.plot(t, z_hat(t)[:,1:], label = 'NN')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e28f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6211b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fadb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75100afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730f086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5849d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1503026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f205e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424e154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1117c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741e513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82cd510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdef0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794c9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4514461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4cd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d79e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f973e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1d986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdfbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77254b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b494e1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e484527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d73db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4882c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aee8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065d028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e07f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbd49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640b7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371c22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defd78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd323d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00e965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27599d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e28b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = X_approx(num_of_hid_layers = 3, units_per_layer = 5, num_out=1, initialization = 0,activation = 'mish')\n",
    "y_hat = Y_approx(num_of_hid_layers = 3, units_per_layer = 5, num_out=1, initialization = 0,activation = 'mish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(t) \n",
    "    x = x_hat(t)\n",
    "    y = y_hat(t)\n",
    "x_prime = tape.gradient(x,t)    \n",
    "y_prime = tape.gradient(y,t)  \n",
    "del tape\n",
    "print(f'x prime {x_prime} \\n y_prime {y_prime}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16060ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    def __init__(self, t,model_x, model_y, x_init, y_init,optim):\n",
    "        self.model_x = model_x\n",
    "        self.model_y = model_y\n",
    "        \n",
    "        self.t = t\n",
    "        self.x_init = x_init\n",
    "        self.y_init = y_init\n",
    "        \n",
    "        self.optim = optim\n",
    "\n",
    "        \n",
    "    def get_derivatives(self,t):\n",
    "        '''\n",
    "        Calculate the derivatives and function that correspond to the ode\n",
    "\n",
    "        input:\n",
    "                t : independent variant\n",
    "\n",
    "        output:\n",
    "                value and corresponding derivatives\n",
    "                \n",
    "        \n",
    "\n",
    "        '''\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(t) \n",
    "            x = self.model_x(t)\n",
    "            y = self.model_y(t)\n",
    "        x_prime = tape.gradient(x,t)    \n",
    "        y_prime = tape.gradient(y,t)   \n",
    "        del tape\n",
    "        \n",
    "        return x,y,x_prime,y_prime   \n",
    "        \n",
    "    def ode_ic(self,t):\n",
    "        x_hat,y_hat,x_prime,y_prime = self.get_derivatives(self.t)\n",
    "       \n",
    "        u1 =  x_prime + x_hat - 6.0*y_hat\n",
    "        u2 =  y_prime - x_hat + 2.0*y_hat\n",
    "#         Initial conditions\n",
    "        x0,y0,x_prime0,y_prime0 = self.get_derivatives(tf.zeros_like(self.t))\n",
    "#        \n",
    "        u3 =  x0-self.x_init\n",
    "        u4 =  y0 - self.y_init\n",
    "\n",
    "        return u1,u2,u3,u4\n",
    "        \n",
    "    def loss(self,t):\n",
    "        u1,u2,u3,u4 = self.ode_ic(t)\n",
    "        mse_x  = tf.reduce_mean(tf.square(u1))\n",
    "        mse_y  = tf.reduce_mean(tf.square(u2))\n",
    "        mse_x0 = tf.reduce_mean(tf.square(u3))\n",
    "        mse_y0 = tf.reduce_mean(tf.square(u4))\n",
    "        return mse_x+mse_y+mse_x0+mse_y0\n",
    "    \n",
    "    \n",
    "    def train_with_adam(self,batch_mode,epochs,batch_size,t):\n",
    "#         which parameters???\n",
    "        @tf.function\n",
    "        def train_and_update(t):\n",
    "    #         print(f'params : \\n {self.model.trainable_variables}')\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(model.trainable_variables)\n",
    "                los  = self.loss(t)\n",
    "            dlossdpa = tape.gradient(los,self.model.trainable_variables) \n",
    "            self.optim.apply_gradients(zip(dlossdpa, self.model.trainable_variables))\n",
    "            return los\n",
    "        \n",
    "        def train_and_update_batch(epochs, batch_size,t):\n",
    "            loss_batch = []\n",
    "            steps_per_epoch = int(t.shape[0]/batch_size)\n",
    "            \n",
    "            initial_loss = train_and_update(t).numpy()\n",
    "            print(f'initial loss : {initial_loss}')\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "#                 print(f'Epoch {epoch} ', end = \".\")\n",
    "                for i in range(steps_per_epoch):\n",
    "                    t_batch = t[i*batch_size:batch_size*(i+1)]\n",
    "                    loss = train_and_update(t_batch).numpy()\n",
    "                    loss_batch.append(loss)\n",
    "                    \n",
    "#                     if i%(steps_per_epoch/10)==0:\n",
    "#                         print(end=\".\")\n",
    "                        \n",
    "                if epoch%1000==0:   \n",
    "                    print(f'loss for epoch {epoch} : {loss}')  \n",
    "            print(f'Final loss : {loss}')    \n",
    "            return loss_batch      \n",
    "                        \n",
    "                        \n",
    "        def train_whole_set(t):                \n",
    "            losses =[]\n",
    "            initial_loss = train_and_update(t).numpy()\n",
    "            print(f'initial loss : {initial_loss}')\n",
    "            losses.append(initial_loss)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "            #     print(f'epoch {epoch}')\n",
    "            #     print('epoch {}'.format(epoch),end = \".\")\n",
    "                loss = train_and_update(t).numpy()\n",
    "                losses.append(loss)\n",
    "            #     print(f'loss : {loss}')\n",
    "\n",
    "                if epoch%1000==0:\n",
    "                    print(f'loss for epoch {epoch} : {loss}') \n",
    "            #         print(end = \".\")\n",
    "            print(f'Final loss : {loss}') \n",
    "            return losses\n",
    "        \n",
    "        if batch_mode == True:\n",
    "            losses = train_and_update_batch(epochs, batch_size,t)\n",
    "        else:\n",
    "            losses = train_whole_set(t)\n",
    "            \n",
    "        return losses   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam()\n",
    "epochs = 5000\n",
    "batch = 10\n",
    "x_int = tf.constant(2.0, dtype = tf.float32)\n",
    "y_int = tf.constant(0.0, dtype = tf.float32)\n",
    "\n",
    "\n",
    "pinn = PINN(t,x_hat, y_hat, x_int, y_int,optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbafb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = pinn.train_with_adam(batch_mode=False,epochs=epochs,batch_size=batch,t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40ed3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
